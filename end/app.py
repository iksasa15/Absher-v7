import cv2
import time
import torch
import numpy as np
import os
import sys
from datetime import datetime
from threading import Thread, Lock
from ultralytics import YOLO
from PIL import Image
import flask
from flask import Flask, request, render_template, jsonify, send_from_directory
import base64
import json
import random
import uuid
from flask_socketio import SocketIO, emit
import logging




# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                   handlers=[logging.StreamHandler()])
logger = logging.getLogger('surveillance-app')




# Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø·Ù„Ù‚ Ù„Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ app.py
base_dir = os.path.abspath(os.path.dirname(__file__))
parent_dir = os.path.dirname(base_dir)  # Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨




logger.info(f"Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: {base_dir}")
logger.info(f"Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø£Ø¨: {parent_dir}")




# Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
required_folders = [
    os.path.join(parent_dir, "static"),
    os.path.join(parent_dir, "static/uploads"),
    os.path.join(parent_dir, "static/processed"),
    os.path.join(parent_dir, "static/captures")
]




for folder in required_folders:
    if not os.path.exists(folder):
        os.makedirs(folder)
        logger.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯: {folder}")
    else:
        logger.info(f"Ø§Ù„Ù…Ø¬Ù„Ø¯ Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ù„ÙØ¹Ù„: {folder}")




# Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ CLIP
try:
    from transformers import CLIPProcessor, CLIPModel
    USE_OPENAI_CLIP = False
    logger.info("âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù… CLIPProcessor Ù…Ù† transformers")
except ImportError:
    try:
        from transformers import CLIPFeatureExtractor as CLIPProcessor, CLIPModel
        USE_OPENAI_CLIP = False
        logger.info("âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù… CLIPFeatureExtractor Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† CLIPProcessor")
    except ImportError:
        try:
            import clip
            USE_OPENAI_CLIP = True
            logger.info("âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© CLIP Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù…Ù† OpenAI")
        except ImportError:
            logger.error("âŒ ÙØ´Ù„ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø£ÙŠ Ø¥ØµØ¯Ø§Ø± Ù…Ù† CLIP. ÙŠØ±Ø¬Ù‰ ØªØ«Ø¨ÙŠØª Ø¥Ø­Ø¯Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª:")
            logger.error("   pip install transformers pillow torch")
            logger.error("   Ø£Ùˆ: pip install git+https://github.com/openai/CLIP.git")
            USE_OPENAI_CLIP = False




# Ø¥Ø¹Ø¯Ø§Ø¯ ØªØ·Ø¨ÙŠÙ‚ Flask Ù…Ø¹ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨ ÙƒØ¯Ù„ÙŠÙ„ Ù„Ù„Ù‚ÙˆØ§Ù„Ø¨ ÙˆØ§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø«Ø§Ø¨ØªØ©
app = Flask(__name__, 
            static_folder=os.path.join(parent_dir, 'static'),
            static_url_path='/static',
            template_folder=parent_dir)




# ØªÙ‡ÙŠØ¦Ø© Socket.IO Ù…Ø¹ Ø¯Ø¹Ù… CORS
socketio = SocketIO(app, cors_allowed_origins="*")




# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
PROCESS_WIDTH = 640
PROCESS_HEIGHT = 360




# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª YOLO
YOLO_CONF = 0.35
YOLO_IOU = 0.35




# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©
UPLOADS_FOLDER = os.path.join(parent_dir, "static/uploads")
PROCESSED_FOLDER = os.path.join(parent_dir, "static/processed")
CAPTURES_FOLDER = os.path.join(parent_dir, "static/captures")




# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
os.makedirs(UPLOADS_FOLDER, exist_ok=True)
os.makedirs(PROCESSED_FOLDER, exist_ok=True)
os.makedirs(CAPTURES_FOLDER, exist_ok=True)




# ÙØ¦Ø§Øª COCO
PERSON_ID = 0
BAG_IDS = {24: "Bag", 26: "Bag", 28: "Bag"}
WEAPON_CLASSES = {43: "Knife", 76: "Scissors"}




# ØªØ³Ù…ÙŠØ§Øª CLIP - ØªÙ… ØªØºÙŠÙŠØ± Ø§Ù„ØªØ±ØªÙŠØ¨
CLIP_LABELS = ["face with mask", "face without mask"]




# Ø§Ù„Ø£Ù„ÙˆØ§Ù† (BGR)
C_GREEN = (0, 255, 0)      # Ù‚Ù†Ø§Ø¹
C_RED = (0, 0, 255)        # Ø¨Ø¯ÙˆÙ† Ù‚Ù†Ø§Ø¹ / Ø£Ø³Ù„Ø­Ø©
C_ORANGE = (0, 165, 255)   # Ø·Ø§Ø¦Ø±Ø© Ø¨Ø¯ÙˆÙ† Ø·ÙŠØ§Ø±
C_YELLOW = (0, 255, 255)   # Ø­Ù‚Ø§Ø¦Ø¨
C_BLUE = (255, 150, 0)     # Ø£Ø´Ø®Ø§Øµ
C_CYAN = (255, 255, 0)     # FPS
C_WHITE = (255, 255, 255)
C_BLACK = (0, 0, 0)




# Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø§Ù„Ù…ÙŠØ© Ù„Ù„Ù†Ù…Ø§Ø°Ø¬
global_models = None




# ØªØªØ¨Ø¹ Ø§Ù„Ù…Ù‡Ø§Ù…
tasks = {}




# Ø­Ø§Ù„Ø© Ø§Ù„Ø¨Ø«
stream_active = False
stream_thread = None
stream_lock = Lock()
active_streams = {}  # ØªØªØ¨Ø¹ Ø¹Ø¯Ø© Ø¨Ø«




def get_dynamic_sizes(width):
    """Ø­Ø³Ø§Ø¨ Ø£Ø­Ø¬Ø§Ù… Ø§Ù„Ø±Ø³Ù… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø·Ø§Ø±"""
    if width >= 1280:
        return 3, 0.9, 2
    elif width >= 960:
        return 2, 0.7, 2
    else:
        return 2, 0.6, 2




def enhance_face(face_img):
    """ØªØ­Ø³ÙŠÙ† ØµÙˆØ±Ø© Ø§Ù„ÙˆØ¬Ù‡ Ù„ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© CLIP"""
    try:
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ LAB
        lab = cv2.cvtColor(face_img, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # ØªØ¹Ø²ÙŠØ² Ù‚Ù†Ø§Ø© Ø§Ù„Ø³Ø·ÙˆØ¹
        l = cv2.equalizeHist(l)
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù‚Ù†ÙˆØ§Øª
        enhanced = cv2.merge([l, a, b])
        return cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙˆØ¬Ù‡: {str(e)}")
        return face_img




def draw_box(frame, x1, y1, x2, y2, color, label, thickness, font_scale, font_thick):
    try:
        cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness)
        size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thick)[0]
        cv2.rectangle(frame, (x1, y1 - size[1] - 12), (x1 + size[0] + 12, y1), color, -1)
        cv2.putText(frame, label, (x1 + 6, y1 - 6), cv2.FONT_HERSHEY_SIMPLEX, font_scale, C_WHITE, font_thick)
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø±Ø³Ù… Ø§Ù„Ù…Ø±Ø¨Ø¹: {str(e)}")




def draw_dashboard(frame, fps, stats, alert):
    try:
        h, w = frame.shape[:2]
        
        bg = (0, 0, 100) if alert else C_BLACK
        cv2.rectangle(frame, (0, 0), (w, 80), bg, -1)
        cv2.line(frame, (0, 80), (w, 80), C_WHITE, 2)
        
        # Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø£ÙˆÙ„
        cv2.putText(frame, f"FPS: {fps:.1f}", (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, C_CYAN, 2)
        cv2.putText(frame, f"Persons: {stats['persons']}", (140, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, C_BLUE, 2)
        cv2.putText(frame, f"Bags: {stats['bags']}", (330, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, C_YELLOW, 2)
        
        # Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø«Ø§Ù†ÙŠ
        cv2.putText(frame, f"Mask: {stats['mask']}", (15, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.65, C_GREEN, 2)
        cv2.putText(frame, f"NoMask: {stats['no_mask']}", (140, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.65, C_RED, 2)
        cv2.putText(frame, f"Weapons: {stats['weapons']}", (290, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.65, C_RED, 2)
        cv2.putText(frame, f"Drones: {stats['drones']}", (460, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.65, C_ORANGE, 2)
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø±Ø³Ù… Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª: {str(e)}")




def draw_alert(frame, alert_type, blink):
    try:
        h, w = frame.shape[:2]
        color = C_ORANGE if "DRONE" in alert_type else C_RED
        
        if blink:
            cv2.rectangle(frame, (0, 0), (w, h), color, 15)
        
        cv2.rectangle(frame, (0, h-60), (w, h), color, -1)
        text = f"!! ALERT: {alert_type} !!"
        cv2.putText(frame, text, (w//2 - 180, h-20), cv2.FONT_HERSHEY_SIMPLEX, 1.0, C_WHITE, 2)
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø±Ø³Ù… Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡: {str(e)}")




def capture_frame(frame, detection_type, bbox):
    """
    Ø§Ù„ØªÙ‚Ø§Ø· ÙˆØ­ÙØ¸ Ø¥Ø·Ø§Ø± Ù…Ø¹ Ø§Ù„ÙƒØ´Ù
    
    Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª:
        frame: Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ
        detection_type: Ù†ÙˆØ¹ Ø§Ù„ÙƒØ´Ù (Knife/Person/Ø¥Ù„Ø®)
        bbox: Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø§Ù„ÙƒØ´Ù (x1, y1, x2, y2)
    
    Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹:
        Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­ÙÙˆØ¸
    """
    try:
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ù…Ø¹ Ø§Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø²Ù…Ù†ÙŠ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        filename = f"{detection_type}_{timestamp}.jpg"
        filepath = os.path.join(CAPTURES_FOLDER, filename)
        
        # Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù…Ø¹ ØµÙ†Ø¯ÙˆÙ‚ Ø£Ø­Ù…Ø±
        x1, y1, x2, y2 = bbox
        frame_copy = frame.copy()
        
        # Ø±Ø³Ù… Ù…Ø±Ø¨Ø¹ Ø£Ø­Ù…Ø± Ø­ÙˆÙ„ Ø§Ù„ÙƒØ´Ù
        cv2.rectangle(frame_copy, (x1, y1), (x2, y2), (0, 0, 255), 4)
        
        # Ø¥Ø¶Ø§ÙØ© Ù†Øµ ØªØ­Ø°ÙŠØ±
        cv2.putText(frame_copy, f"DETECTED: {detection_type}", (x1, y1 - 15),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø²Ù…Ù†ÙŠ Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©
        time_text = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        cv2.putText(frame_copy, time_text, (10, frame_copy.shape[0] - 20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø©
        cv2.imwrite(filepath, frame_copy)
        
        logger.info(f"âš ï¸  ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø· {detection_type}: {filepath}")
        
        # Ø¥Ø±Ø³Ø§Ù„ ØªÙ†Ø¨ÙŠÙ‡ Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø§Ù„Ù…ØªØµÙ„ÙŠÙ†
        alert_data = {
            'type': detection_type,
            'path': f"/static/captures/{filename}",
            'confidence': 98,  # Ù…ÙƒØ§Ù†
            'timestamp': datetime.now().strftime("%H:%M:%S")
        }
        socketio.emit('alert', alert_data)
        
        return f"/static/captures/{filename}"
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„Ø¥Ø·Ø§Ø±: {str(e)}")
        return ""




def load_models():
    """ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ´Ù"""
    global global_models
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ø­Ù…Ù„Ø© Ø¨Ø§Ù„ÙØ¹Ù„ØŒ Ø£Ø¹Ø¯Ù‡Ø§
    if global_models is not None:
        return global_models
        
    logger.info("=" * 70)
    logger.info("   ğŸ” Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„ - ÙˆØ­Ø¯Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ")
    logger.info("=" * 70)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"ğŸ–¥ï¸  Ø§Ù„Ø¬Ù‡Ø§Ø²: {device.upper()}")
    logger.info("-" * 70)
    
    # ØªØ­Ù…ÙŠÙ„ YOLOv8
    logger.info("â³ 1/2: ØªØ­Ù…ÙŠÙ„ YOLOv8 (Ø§Ù„Ø£Ø´Ø®Ø§Øµ + Ø§Ù„Ø­Ù‚Ø§Ø¦Ø¨ + Ø§Ù„Ø£Ø³Ù„Ø­Ø©)...")
    yolo_path = os.path.join(parent_dir, "yolov8x.pt")
    try:
        if os.path.exists(yolo_path):
            logger.info(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù yolo: {yolo_path}")
            yolo = YOLO(yolo_path)
            logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ YOLOv8x!")
        else:
            logger.warning(f"Ù…Ù„Ù yolo ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ: {yolo_path}, Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¶Ù…Ù†...")
            yolo = YOLO("yolov8n.pt")
            logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ YOLOv8n!")
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ YOLO: {str(e)}")
        logger.warning("âš ï¸ YOLOv8x ØºÙŠØ± Ù…ØªÙˆÙØ±ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… YOLOv8n...")
        yolo = YOLO("yolov8n.pt")
        logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ YOLOv8n!")
    
    # ØªØ­Ù…ÙŠÙ„ CLIP ÙˆØªØ³Ù„Ø³Ù„ Ø§Ù„ÙˆØ¬Ù‡
    logger.info("â³ 2/2: ØªØ­Ù…ÙŠÙ„ CLIP + Haar Cascade (Ø§Ù„Ø£Ù‚Ù†Ø¹Ø©)...")
    face_cascade = cv2.CascadeClassifier(
        cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
    )
    
    clip_model = None
    clip_proc = None
    use_openai_clip = False
    
    try:
        if USE_OPENAI_CLIP:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenAI CLIP Ø§Ù„Ø£ØµÙ„ÙŠ
            clip_model, clip_proc = clip.load("ViT-L/14", device=device)
            logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ CLIP-Large (OpenAI)!")
            use_openai_clip = True
        else:
            try:
                clip_proc = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
                clip_model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
                logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ CLIP-Large (Transformers)!")
            except Exception as e:
                logger.warning(f"âš ï¸ CLIP-Large ØºÙŠØ± Ù…ØªÙˆÙØ±: {str(e)}, Ø§Ø³ØªØ®Ø¯Ø§Ù… CLIP-Base...")
                clip_proc = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
                clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
                logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ CLIP-Base (Transformers)!")
    except Exception as e:
        logger.error(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ CLIP: {str(e)}")
        logger.warning("âš ï¸ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø¨Ø¯ÙˆÙ† ÙƒØ´Ù Ø§Ù„Ù‚Ù†Ø§Ø¹...")
    
    if not use_openai_clip and clip_model is not None:
        clip_model.to(device).eval()
    
    logger.info("-" * 70)
    logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬!")
    logger.info("=" * 70)
    
    global_models = {
        'yolo': yolo,
        'face_cascade': face_cascade,
        'clip_model': clip_model,
        'clip_proc': clip_proc,
        'device': device,
        'use_openai_clip': use_openai_clip
    }
    
    return global_models




def process_video_thread(video_path, task_id):
    """
    Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù ÙÙŠØ¯ÙŠÙˆ ÙÙŠ Ù…Ø¤Ø´Ø± ØªØ±Ø§Ø¨Ø· Ø®Ù„ÙÙŠ ÙˆØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù‡Ù…Ø©
    
    Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª:
        video_path: Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ù…Ø¯Ø®Ù„
        task_id: Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ù‡Ù…Ø© Ù„ØªØªØ¨Ø¹ Ø§Ù„ØªÙ‚Ø¯Ù…
    """
    # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù‡Ù…Ø© Ø¥Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø©
    tasks[task_id]['status'] = 'processing'
    tasks[task_id]['progress'] = 0
    
    try:
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
        if not os.path.exists(video_path):
            logger.error(f"Ø®Ø·Ø£: Ù…Ù„Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {video_path}")
            tasks[task_id]['status'] = 'error'
            tasks[task_id]['error'] = "Ù…Ù„Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯"
            return
        
        logger.info(f"Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {video_path} Ù„Ù„Ù…Ù‡Ù…Ø©: {task_id}")
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        models = load_models()
        
        # ÙØªØ­ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            logger.error(f"ØªØ¹Ø°Ø± ÙØªØ­ Ù…Ù„Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {video_path}")
            tasks[task_id]['status'] = 'error'
            tasks[task_id]['error'] = "ØªØ¹Ø°Ø± ÙØªØ­ Ù…Ù„Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ"
            return
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        logger.info(f"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {width}x{height}, FPS: {fps}, Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª: {frame_count}")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø³Ø§Ø± Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        output_filename = f"processed_{os.path.basename(video_path)}"
        output_path = os.path.join(PROCESSED_FOLDER, output_filename)
        
        # Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§ØªØ¨ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        box_thick, font_scale, font_thick = get_dynamic_sizes(width)
        
        # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªØ­Ø¬ÙŠÙ… Ù„ÙƒØ´Ù YOLO
        scale_x = width / PROCESS_WIDTH
        scale_y = height / PROCESS_HEIGHT
        
        # Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        total_persons = 0
        total_bags = 0
        total_weapons = 0
        total_masks = 0
        total_no_masks = 0
        
        # ØªØªØ¨Ø¹ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„ÙØ±ÙŠØ¯ (ØªØªØ¨Ø¹ Ø¨Ø³ÙŠØ·)
        seen_persons = set()
        seen_bags = set()
        seen_weapons = set()
        
        # Ø§Ù„ØªÙ‚Ø§Ø·Ø§Øª Ù„Ù„ÙƒØ´Ù
        captures = []
        
        # ÙƒØ´Ù Ø§Ù„ÙˆØ¬Ù‡ - ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø¥Ø°Ø§ ÙƒØ§Ù† CLIP Ù…ØªØ§Ø­Ù‹Ø§
        if models['clip_model'] is not None:
            face_detector = models['face_cascade']
            clip_model = models['clip_model']
            clip_proc = models['clip_proc']
            use_openai_clip = models['use_openai_clip']
            device = models['device']
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª
        frame_idx = 0
        process_fps = 0
        start_time = time.time()
        
        # Ø­Ø§Ù„Ø© Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡
        alert_active = False
        alert_type = ""
        blink = True
        blink_timer = time.time()
        
        # Ø§Ù„ØªÙ‚Ø§Ø· ÙƒÙ„ 3 Ø«ÙˆØ§Ù†Ù Ù„Ù„ÙƒØ´Ù Ø§Ù„ÙØ±ÙŠØ¯
        last_capture_time = 0
        CAPTURE_INTERVAL = 3  # Ø«ÙˆØ§Ù†Ù
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù„ÙƒÙ„ Ø¥Ø·Ø§Ø±
        stats = {
            'persons': 0,
            'bags': 0,
            'mask': 0,
            'no_mask': 0,
            'weapons': 0,
            'drones': 0  # Ù…ÙƒØ§Ù†ØŒ ØºÙŠØ± Ù…Ù†ÙØ° ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù†Ø³Ø®Ø©
        }
        
        # Ø­Ù„Ù‚Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # ØªØ­Ø¯ÙŠØ« FPS ÙˆØ­Ø§Ù„Ø© Ø§Ù„ÙˆÙ…ÙŠØ¶
            frame_idx += 1
            now = time.time()
            elapsed = now - start_time
            if elapsed > 1:
                process_fps = frame_idx / elapsed
            
            if now - blink_timer >= 0.3:
                blink = not blink
                blink_timer = now
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù…
            progress = min(99, int((frame_idx / frame_count) * 100))
            tasks[task_id]['progress'] = progress
            
            # Ø¥Ø±Ø³Ø§Ù„ ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù… Ø¹Ø¨Ø± Socket.IO
            if frame_idx % 10 == 0:  # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØªÙ‚Ø¯Ù… ÙƒÙ„ 10 Ø¥Ø·Ø§Ø±Ø§Øª Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ÙÙŠØ¶Ø§Ù†
                socketio.emit('task_progress', {
                    'task_id': task_id,
                    'progress': progress,
                    'stats': stats
                })
            
            # Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… YOLO (ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø¬Ù… Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø³Ø±Ø¹)
            small = cv2.resize(frame, (PROCESS_WIDTH, PROCESS_HEIGHT))
            results = models['yolo'](small, conf=YOLO_CONF, iou=YOLO_IOU, verbose=False)
            
            # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¥Ø·Ø§Ø±
            stats = {
                'persons': 0,
                'bags': 0,
                'mask': 0,
                'no_mask': 0,
                'weapons': 0,
                'drones': 0
            }
            
            persons = []
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù†ØªØ§Ø¦Ø¬ YOLO
            for r in results:
                if r.boxes is None:
                    continue
                
                for box in r.boxes:
                    cid = int(box.cls[0])
                    # ØªØºÙŠÙŠØ± Ø­Ø¬Ù… Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø¥Ù„Ù‰ Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©
                    x1 = int(box.xyxy[0][0] * scale_x)
                    y1 = int(box.xyxy[0][1] * scale_y)
                    x2 = int(box.xyxy[0][2] * scale_x)
                    y2 = int(box.xyxy[0][3] * scale_y)
                    conf = float(box.conf[0])
                    
                    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø±ÙƒØ² Ù„Ù„ØªØªØ¨Ø¹
                    center_x = (x1 + x2) // 2
                    center_y = (y1 + y2) // 2
                    
                    # Ø­Ù‚Ø§Ø¦Ø¨
                    if cid in BAG_IDS:
                        stats['bags'] += 1
                        total_bags += 1
                        seen_bags.add((center_x // 50, center_y // 50))  # Ø¥Ù„ØºØ§Ø¡ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ø¨Ø³ÙŠØ·
                        
                        draw_box(frame, x1, y1, x2, y2, C_YELLOW, f"{BAG_IDS[cid]}: {conf:.2f}", 
                                box_thick, font_scale, font_thick)
                    
                    # Ø£Ø´Ø®Ø§Øµ
                    elif cid == PERSON_ID:
                        stats['persons'] += 1
                        total_persons += 1
                        seen_persons.add((center_x // 50, center_y // 50))  # Ø¥Ù„ØºØ§Ø¡ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ø¨Ø³ÙŠØ·
                        
                        cv2.rectangle(frame, (x1, y1), (x2, y2), C_BLUE, box_thick)
                        cv2.putText(frame, f"Person: {conf:.2f}", (x1, y2+20), 
                                    cv2.FONT_HERSHEY_SIMPLEX, font_scale, C_BLUE, font_thick)
                        
                        # Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù‚Ù†Ø§Ø¹
                        persons.append((x1, y1, x2, y2))
                    
                    # Ø£Ø³Ù„Ø­Ø©
                    elif cid in WEAPON_CLASSES:
                        stats['weapons'] += 1
                        total_weapons += 1
                        weapon_type = WEAPON_CLASSES[cid]
                        seen_weapons.add((weapon_type, center_x // 50, center_y // 50))
                        
                        draw_box(frame, x1, y1, x2, y2, C_RED, f"{weapon_type}: {conf:.2f}", 
                                box_thick + 2, font_scale + 0.2, font_thick)
                        
                        alert_active = True
                        alert_type = weapon_type.upper()
                        
                        # Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„ÙƒØ´Ù Ø¥Ø°Ø§ Ù…Ø± Ø§Ù„ÙØ§ØµÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ
                        if now - last_capture_time >= CAPTURE_INTERVAL:
                            capture_path = capture_frame(frame, weapon_type, (x1, y1, x2, y2))
                            captures.append({
                                'type': weapon_type,
                                'path': capture_path,
                                'confidence': round(conf * 100),
                                'timestamp': datetime.now().strftime("%H:%M:%S")
                            })
                            last_capture_time = now
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ¬ÙˆÙ‡ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù‚Ù†Ø§Ø¹ Ø¥Ø°Ø§ ÙƒØ§Ù† CLIP Ù…ØªØ§Ø­Ù‹Ø§
            if models['clip_model'] is not None:
                for (px1, py1, px2, py2) in persons:
                    roi = frame[py1:py2, px1:px2]
                    if roi.size == 0:
                        continue
                    
                    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
                    faces = face_detector.detectMultiScale(gray, 1.15, 3, minSize=(25, 25))
                    
                    for (fx, fy, fw, fh) in faces:
                        if fw < 30:
                            continue
                        
                        face = roi[fy:fy+fh, fx:fx+fw]
                        if face.size < 400:
                            continue
                        
                        # ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØ±Ø©
                        face = enhance_face(face)
                        
                        small = cv2.resize(face, (56, 56))
                        rgb = cv2.cvtColor(small, cv2.COLOR_BGR2RGB)
                        pil = Image.fromarray(rgb)
                        
                        if use_openai_clip:
                            # Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenAI CLIP Ø§Ù„Ø£ØµÙ„ÙŠ
                            image = clip_proc(pil).unsqueeze(0).to(device)
                            text = clip.tokenize(CLIP_LABELS).to(device)
                            
                            with torch.no_grad():
                                logits_per_image, _ = clip_model(image, text)
                                probs = logits_per_image.softmax(dim=-1)
                        else:
                            # Ø§Ø³ØªØ®Ø¯Ø§Ù… HuggingFace transformers CLIP
                            inputs = clip_proc(
                                text=CLIP_LABELS, images=pil,
                                return_tensors="pt", padding=True
                            ).to(device)
                            
                            with torch.no_grad():
                                out = clip_model(**inputs)
                                probs = out.logits_per_image.softmax(dim=1)
                        
                        idx = probs.argmax().item()
                        conf = probs[0][idx].item()
                        has_mask = (idx == 0)  # ØªÙ… ØªØºÙŠÙŠØ±Ù‡ Ù…Ù† 1 Ø¥Ù„Ù‰ 0 Ù„Ø£Ù†Ù†Ø§ Ø¹ÙƒØ³Ù†Ø§ ØªØ±ØªÙŠØ¨ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª
                        
                        color = C_GREEN if has_mask else C_RED
                        label = "NO MASK" if has_mask else "MASK"
                        
                        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                        if has_mask:
                            stats['mask'] += 1
                            total_masks += 1
                        else:
                            stats['no_mask'] += 1
                            total_no_masks += 1
                            
                            # Ø§Ù„ØªÙ‚Ø§Ø· ÙƒØ´Ù Ø¨Ø¯ÙˆÙ† Ù‚Ù†Ø§Ø¹ Ø¥Ø°Ø§ Ù…Ø± Ø§Ù„ÙØ§ØµÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ
                            if now - last_capture_time >= CAPTURE_INTERVAL:
                                capture_path = capture_frame(frame, "NoMask", (px1+fx, py1+fy, px1+fx+fw, py1+fy+fh))
                                captures.append({
                                    'type': 'NoMask',
                                    'path': capture_path,
                                    'confidence': round(conf * 100),
                                    'timestamp': datetime.now().strftime("%H:%M:%S")
                                })
                                last_capture_time = now
                        
                        draw_box(frame, px1+fx, py1+fy, px1+fx+fw, py1+fy+fh, color, 
                                f"{label}: {conf:.2f}", box_thick, font_scale, font_thick)
            
            # Ø±Ø³Ù… Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
            draw_dashboard(frame, process_fps, stats, alert_active)
            
            # Ø±Ø³Ù… Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù†Ø´Ø·Ù‹Ø§
            if alert_active:
                draw_alert(frame, alert_type, blink)
            
            # ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¥Ø·Ø§Ø±
            out.write(frame)
            
            # ÙƒÙ„ 30 Ø¥Ø·Ø§Ø±Ù‹Ø§ØŒ Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø·Ø§Ø± Ø¹Ø¨Ø± Socket.IO
            if frame_idx % 30 == 0:
                _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 80])
                frame_base64 = base64.b64encode(buffer).decode('utf-8')
                socketio.emit('video_frame', {
                    'task_id': task_id,
                    'frame': frame_base64,
                    'frame_number': frame_idx,
                    'stats': stats
                })
        
        # ØªØ­Ø±ÙŠØ± Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
        cap.release()
        out.release()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ù„Ø®Øµ
        unique_persons = len(seen_persons)
        unique_bags = len(seen_bags)
        unique_weapons = len(seen_weapons)
        
        # Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ù…ØµØºØ±Ø©
        thumbnail_path = os.path.join(PROCESSED_FOLDER, f"thumb_{os.path.basename(video_path)}.jpg")
        cap = cv2.VideoCapture(output_path)
        ret, thumb = cap.read()
        if ret:
            cv2.imwrite(thumbnail_path, thumb)
        cap.release()
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        tasks[task_id].update({
            'status': 'completed',
            'progress': 100,
            'output_path': f"/static/processed/{output_filename}",
            'thumbnail': f"/static/processed/thumb_{os.path.basename(video_path)}.jpg",
            'captures': captures,
            'stats': {
                'duration': frame_idx/fps if fps > 0 else 0,
                'frames': frame_idx,
                'persons': {
                    'unique': unique_persons,
                    'total': total_persons
                },
                'bags': {
                    'unique': unique_bags,
                    'total': total_bags
                },
                'weapons': {
                    'unique': unique_weapons, 
                    'total': total_weapons
                },
                'mask': total_masks,
                'no_mask': total_no_masks,
                'fps': process_fps
            }
        })
        
        logger.info(f"Ø§ÙƒØªÙ…Ù„Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù„Ù„Ù…Ù‡Ù…Ø©: {task_id}")
        
        # Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø¹Ø§Ø± Ø§Ù„Ø§ÙƒØªÙ…Ø§Ù„ Ø¹Ø¨Ø± Socket.IO
        socketio.emit('task_completed', {
            'task_id': task_id,
            'output_path': f"/static/processed/{output_filename}",
            'thumbnail': f"/static/processed/thumb_{os.path.basename(video_path)}.jpg",
            'stats': tasks[task_id]['stats']
        })
    
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù„Ù„Ù…Ù‡Ù…Ø© {task_id}: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        tasks[task_id]['status'] = 'error'
        tasks[task_id]['error'] = str(e)
        socketio.emit('task_error', {
            'task_id': task_id,
            'error': str(e)
        })




def process_stream(stream_id, source_type, source_path=None, rtsp_url=None):
    """
    Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø« ÙÙŠØ¯ÙŠÙˆ (ÙƒØ§Ù…ÙŠØ±Ø§ ÙˆÙŠØ¨ØŒ Ù…Ù„ÙØŒ Ø£Ùˆ RTSP)
    
    Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª:
        stream_id: Ù…Ø¹Ø±Ù ÙØ±ÙŠØ¯ Ù„Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø«
        source_type: Ù†ÙˆØ¹ Ø§Ù„Ù…ØµØ¯Ø± ("webcam"ØŒ "file"ØŒ "rtsp")
        source_path: Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ (Ø¥Ø°Ø§ ÙƒØ§Ù† source_type Ù‡Ùˆ "file")
        rtsp_url: Ø¹Ù†ÙˆØ§Ù† URL Ù„Ù€ RTSP (Ø¥Ø°Ø§ ÙƒØ§Ù† source_type Ù‡Ùˆ "rtsp")
    """
    try:
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        models = load_models()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù…ØµØ¯Ø±
        if source_type == "webcam":
            cap = cv2.VideoCapture(0)  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        elif source_type == "file" and source_path:
            if not os.path.exists(source_path):
                logger.error(f"Ù…Ù„Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {source_path}")
                active_streams[stream_id]['status'] = 'error'
                active_streams[stream_id]['error'] = f"Ù…Ù„Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {source_path}"
                return
            cap = cv2.VideoCapture(source_path)
        elif source_type == "rtsp" and rtsp_url:
            cap = cv2.VideoCapture(rtsp_url)
        else:
            active_streams[stream_id]['status'] = 'error'
            active_streams[stream_id]['error'] = "Ù†ÙˆØ¹ Ù…ØµØ¯Ø± ØºÙŠØ± ØµØ§Ù„Ø­ Ø£Ùˆ Ù…Ø¹Ù„Ù…Ø§Øª Ù…ÙÙ‚ÙˆØ¯Ø©"
            return
        
        if not cap.isOpened():
            active_streams[stream_id]['status'] = 'error'
            active_streams[stream_id]['error'] = f"ÙØ´Ù„ ÙÙŠ ÙØªØ­ Ù…ØµØ¯Ø± {source_type}"
            return
        
        # ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¯Ù‚Ø© Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„ÙˆÙŠØ¨
        if source_type == "webcam":
            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 360)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        box_thick, font_scale, font_thick = get_dynamic_sizes(width)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        frame_count = 0
        process_fps = 0
        start_time = time.time()
        alert_active = False
        alert_type = ""
        blink = True
        blink_timer = time.time()
        last_capture_time = 0
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù„Ù„ØªØªØ¨Ø¹
        stats = {
            'persons': 0,
            'bags': 0,
            'mask': 0,
            'no_mask': 0,
            'weapons': 0,
            'drones': 0
        }
        
        # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ø¨Ø«
        active_streams[stream_id]['status'] = 'streaming'
        logger.info(f"âœ… Ø¨Ø¯Ø£ Ø§Ù„Ø¨Ø« {stream_id} ({source_type})")
        
        # Ø­Ù„Ù‚Ø© Ø§Ù„Ø¨Ø« Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        while stream_id in active_streams and active_streams[stream_id]['status'] == 'streaming':
            ret, frame = cap.read()
            
            if not ret:
                # Ù„Ù„Ù…Ù„ÙØ§ØªØŒ Ø§Ø±Ø¬Ø¹ Ø¥Ù„Ù‰ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
                if source_type == "file":
                    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                    continue
                else:
                    # Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„ÙˆÙŠØ¨/RTSPØŒ Ø§Ù†ØªÙ‡ÙŠ Ø¥Ø°Ø§ Ù„Ù… Ù†ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø±
                    break
            
            # ØªØ­Ø¯ÙŠØ« FPS
            frame_count += 1
            now = time.time()
            elapsed = now - start_time
            if elapsed >= 1:
                process_fps = frame_count / elapsed
                frame_count = 0
                start_time = now
            
            # ØªØ­Ø¯ÙŠØ« Ù…Ø¤Ù‚Øª Ø§Ù„ÙˆÙ…ÙŠØ¶
            if now - blink_timer >= 0.3:
                blink = not blink
                blink_timer = now
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… YOLO
            results = models['yolo'](frame, conf=YOLO_CONF, iou=YOLO_IOU, verbose=False)
            
            # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù„Ù‡Ø°Ø§ Ø§Ù„Ø¥Ø·Ø§Ø±
            stats = {
                'persons': 0,
                'bags': 0,
                'mask': 0,
                'no_mask': 0,
                'weapons': 0,
                'drones': 0
            }
            
            persons = []
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù†ØªØ§Ø¦Ø¬ YOLO
            for r in results:
                if r.boxes is None:
                    continue
                
                for box in r.boxes:
                    cid = int(box.cls[0])
                    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª
                    x1 = int(box.xyxy[0][0])
                    y1 = int(box.xyxy[0][1])
                    x2 = int(box.xyxy[0][2])
                    y2 = int(box.xyxy[0][3])
                    conf = float(box.conf[0])
                    
                    # Ø­Ù‚Ø§Ø¦Ø¨
                    if cid in BAG_IDS:
                        stats['bags'] += 1
                        draw_box(frame, x1, y1, x2, y2, C_YELLOW, f"{BAG_IDS[cid]}: {conf:.2f}", 
                                box_thick, font_scale, font_thick)
                    
                    # Ø£Ø´Ø®Ø§Øµ
                    elif cid == PERSON_ID:
                        stats['persons'] += 1
                        cv2.rectangle(frame, (x1, y1), (x2, y2), C_BLUE, box_thick)
                        cv2.putText(frame, f"Person: {conf:.2f}", (x1, y2+20), 
                                    cv2.FONT_HERSHEY_SIMPLEX, font_scale, C_BLUE, font_thick)
                        
                        # Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù‚Ù†Ø§Ø¹
                        persons.append((x1, y1, x2, y2))
                    
                    # Ø£Ø³Ù„Ø­Ø©
                    elif cid in WEAPON_CLASSES:
                        stats['weapons'] += 1
                        weapon_type = WEAPON_CLASSES[cid]
                        
                        draw_box(frame, x1, y1, x2, y2, C_RED, f"{weapon_type}: {conf:.2f}", 
                                box_thick + 2, font_scale + 0.2, font_thick)
                        
                        alert_active = True
                        alert_type = weapon_type.upper()
                        
                        # Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„ÙƒØ´Ù Ø¥Ø°Ø§ Ù…Ø± Ø§Ù„ÙØ§ØµÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ
                        if now - last_capture_time >= 3:
                            capture_path = capture_frame(frame, weapon_type, (x1, y1, x2, y2))
                            last_capture_time = now
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ¬ÙˆÙ‡ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù‚Ù†Ø§Ø¹ Ø¥Ø°Ø§ ÙƒØ§Ù† CLIP Ù…ØªØ§Ø­Ù‹Ø§
            if models['clip_model'] is not None:
                face_detector = models['face_cascade']
                clip_model = models['clip_model']
                clip_proc = models['clip_proc']
                use_openai_clip = models['use_openai_clip']
                device = models['device']
                
                for (px1, py1, px2, py2) in persons:
                    roi = frame[py1:py2, px1:px2]
                    if roi.size == 0:
                        continue
                    
                    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
                    faces = face_detector.detectMultiScale(gray, 1.15, 3, minSize=(25, 25))
                    
                    for (fx, fy, fw, fh) in faces:
                        if fw < 30:
                            continue
                        
                        face = roi[fy:fy+fh, fx:fx+fw]
                        if face.size < 400:
                            continue
                        
                        # ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØ±Ø©
                        face = enhance_face(face)
                        
                        small = cv2.resize(face, (56, 56))
                        rgb = cv2.cvtColor(small, cv2.COLOR_BGR2RGB)
                        pil = Image.fromarray(rgb)
                        
                        if use_openai_clip:
                            # Ø§Ø³ØªØ®Ø¯Ø§Ù… CLIP Ø§Ù„Ø£ØµÙ„ÙŠ Ù…Ù† OpenAI
                            image = clip_proc(pil).unsqueeze(0).to(device)
                            text = clip.tokenize(CLIP_LABELS).to(device)
                            
                            with torch.no_grad():
                                logits_per_image, _ = clip_model(image, text)
                                probs = logits_per_image.softmax(dim=-1)
                        else:
                            # Ø§Ø³ØªØ®Ø¯Ø§Ù… HuggingFace transformers CLIP
                            inputs = clip_proc(
                                text=CLIP_LABELS, images=pil,
                                return_tensors="pt", padding=True
                            ).to(device)
                            
                            with torch.no_grad():
                                out = clip_model(**inputs)
                                probs = out.logits_per_image.softmax(dim=1)
                        
                        idx = probs.argmax().item()
                        conf = probs[0][idx].item()
                        has_mask = (idx == 0)  # ØªÙ… ØªØºÙŠÙŠØ±Ù‡ Ù…Ù† 1 Ø¥Ù„Ù‰ 0 Ù„Ø£Ù†Ù†Ø§ Ø¹ÙƒØ³Ù†Ø§ ØªØ±ØªÙŠØ¨ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª
                        
                        color = C_GREEN if has_mask else C_RED
                        label = "NO MASK" if has_mask else "MASK"
                        
                        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                        if has_mask:
                            stats['mask'] += 1
                        else:
                            stats['no_mask'] += 1
                            
                            # Ø§Ù„ØªÙ‚Ø§Ø· ÙƒØ´Ù Ø¨Ø¯ÙˆÙ† Ù‚Ù†Ø§Ø¹ Ø¥Ø°Ø§ Ù…Ø± Ø§Ù„ÙØ§ØµÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ
                            if now - last_capture_time >= 3:
                                capture_path = capture_frame(frame, "NoMask", (px1+fx, py1+fy, px1+fx+fw, py1+fy+fh))
                                last_capture_time = now
                        
                        draw_box(frame, px1+fx, py1+fy, px1+fx+fw, py1+fy+fh, color, 
                                f"{label}: {conf:.2f}", box_thick, font_scale, font_thick)
            
            # Ø±Ø³Ù… Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
            draw_dashboard(frame, process_fps, stats, alert_active)
            
            # Ø±Ø³Ù… Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù†Ø´Ø·Ù‹Ø§
            if alert_active:
                draw_alert(frame, alert_type, blink)
            
            # Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø·Ø§Ø± Ø¹Ø¨Ø± Socket.IO
            _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 70])
            frame_base64 = base64.b64encode(buffer).decode('utf-8')
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ camera_id Ù…Ù† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨Ø«
            camera_id = active_streams[stream_id].get('name', f"Stream {stream_id}")
            
            # Ø¨Ø« Ø§Ù„Ø¥Ø·Ø§Ø± Ù…Ø¹ camera_id Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
            socketio.emit('stream_frame', {
                'stream_id': stream_id,
                'camera_id': camera_id,  # Ù…Ù‡Ù… Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
                'frame': frame_base64,
                'stats': stats,
                'fps': round(process_fps, 1),
                'detections': {
                    'person_count': stats['persons'],
                    'bag_count': stats['bags'],
                    'weapon_count': stats['weapons'],
                    'distance': random.randint(70, 120),  # Ù…Ø³Ø§ÙØ© Ù…Ø­Ø§ÙƒØ§Ø©
                    'signal': random.randint(70, 95),     # Ù‚ÙˆØ© Ø¥Ø´Ø§Ø±Ø© Ù…Ø­Ø§ÙƒØ§Ø©
                    'confidence': {
                        'person': random.randint(88, 96), # Ø«Ù‚Ø© Ù…Ø­Ø§ÙƒØ§Ø©
                        'bag': random.randint(76, 89)     # Ø«Ù‚Ø© Ù…Ø­Ø§ÙƒØ§Ø©
                    }
                }
            })
            
            # Ø¨Ø« Ø£ÙŠØ¶Ù‹Ø§ Ø¹Ù„Ù‰ Ù‚Ù†Ø§Ø© Ù…Ø­Ø¯Ø¯Ø©
            socketio.emit(f'stream_frame_{stream_id}', {
                'frame': frame_base64,
                'stats': stats,
                'fps': round(process_fps, 1)
            })
            
            # ØªÙ‚Ù„ÙŠÙ„ Ø³Ø±Ø¹Ø© Ø§Ù„Ø¨Ø« Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø²Ø§Ø¦Ø¯ Ø¹Ù„Ù‰ Socket.IO
            time.sleep(0.05)
        
        # ØªØ­Ø±ÙŠØ± Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
        cap.release()
        
        # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ø¨Ø«
        if stream_id in active_streams:
            active_streams[stream_id]['status'] = 'stopped'
        
        logger.info(f"âœ… ØªÙˆÙ‚Ù Ø§Ù„Ø¨Ø« {stream_id}")
    
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨Ø« {stream_id}: {str(e)}")
        if stream_id in active_streams:
            active_streams[stream_id]['status'] = 'error'
            active_streams[stream_id]['error'] = str(e)
        
        socketio.emit(f'stream_error_{stream_id}', {'error': str(e)})
        socketio.emit('stream_error', {'stream_id': stream_id, 'camera_id': active_streams[stream_id].get('name', ''), 'message': str(e)})




def start_stream(source_type, source_path=None, rtsp_url=None, name=None):
    """
    Ø¨Ø¯Ø¡ Ø¨Ø« Ø¬Ø¯ÙŠØ¯ ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ù…Ø¹Ø±ÙÙ‡
    
    Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª:
        source_type: Ù†ÙˆØ¹ Ø§Ù„Ù…ØµØ¯Ø± ("webcam"ØŒ "file"ØŒ "rtsp")
        source_path: Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ (Ø¥Ø°Ø§ ÙƒØ§Ù† source_type Ù‡Ùˆ "file")
        rtsp_url: Ø¹Ù†ÙˆØ§Ù† URL Ù„Ù€ RTSP (Ø¥Ø°Ø§ ÙƒØ§Ù† source_type Ù‡Ùˆ "rtsp")
        name: Ø§Ø³Ù… ÙˆØ¯ÙŠ Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù„Ù„Ø¨Ø«
    
    Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹:
        Ù…Ø¹Ø±Ù Ø§Ù„Ø¨Ø«
    """
    stream_id = str(uuid.uuid4())
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø¨Ø«
    stream_info = {
        'id': stream_id,
        'name': name or f"Stream {len(active_streams) + 1}",
        'type': source_type,
        'status': 'starting',
        'created': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    }
    
    if source_type == "file" and source_path:
        stream_info['source_path'] = source_path
    elif source_type == "rtsp" and rtsp_url:
        stream_info['rtsp_url'] = rtsp_url
    
    active_streams[stream_id] = stream_info
    
    # Ø¥Ø®Ø·Ø§Ø± Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø¨Ø£Ù† Ø§Ù„Ø¨Ø« Ø¨Ø¯Ø£
    socketio.emit('stream_started', {
        'stream_id': stream_id, 
        'camera_id': name,  # Ù…Ù‡Ù… Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
        'status': 'starting'
    })
    
    # Ø¨Ø¯Ø¡ Ù…Ø¤Ø´Ø± ØªØ±Ø§Ø¨Ø· Ø§Ù„Ø¨Ø«
    stream_thread = Thread(
        target=process_stream, 
        args=(stream_id, source_type, source_path, rtsp_url)
    )
    stream_thread.daemon = True
    stream_thread.start()
    
    return stream_id




def stop_stream(stream_id):
    """
    Ø¥ÙŠÙ‚Ø§Ù Ø¨Ø« Ø­Ø³Ø¨ Ø§Ù„Ù…Ø¹Ø±Ù
    
    Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª:
        stream_id: Ù…Ø¹Ø±Ù Ø§Ù„Ø¨Ø« Ø§Ù„Ù…Ø±Ø§Ø¯ Ø¥ÙŠÙ‚Ø§ÙÙ‡
        
    Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹:
        True Ø¥Ø°Ø§ Ù†Ø¬Ø­ØŒ False Ø®Ù„Ø§Ù Ø°Ù„Ùƒ
    """
    if stream_id in active_streams:
        active_streams[stream_id]['status'] = 'stopping'
        time.sleep(0.5)  # Ø¥Ø¹Ø·Ø§Ø¡ Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ±Ø§Ø¨Ø· ÙˆÙ‚ØªÙ‹Ø§ Ù„Ù„Ø®Ø±ÙˆØ¬
        
        # Ø¥Ø®Ø·Ø§Ø± Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø¨Ø£Ù† Ø§Ù„Ø¨Ø« ØªÙˆÙ‚Ù
        socketio.emit('stream_stopped', {
            'stream_id': stream_id,
            'camera_id': active_streams[stream_id].get('name', ''),
            'status': 'stopped'
        })
        
        return True
    return False




# Ø¥Ø¶Ø§ÙØ© Ù…Ø³Ø§Ø±Ø§Øª Ù„Ù…Ù„ÙØ§Øª CSS Ùˆ JS ÙˆØ§Ù„ØµÙˆØ±
@app.route('/css/<path:filename>')
def serve_css(filename):
    return send_from_directory(os.path.join(parent_dir, 'css'), filename)




@app.route('/js/<path:filename>')
def serve_js(filename):
    return send_from_directory(os.path.join(parent_dir, 'js'), filename)




@app.route('/img/<path:filename>')
def serve_img(filename):
    return send_from_directory(os.path.join(parent_dir, 'img'), filename)




@app.route('/<path:filename>.png')
def serve_png(filename):
    return send_from_directory(parent_dir, f"{filename}.png")




@app.route('/weapon_captures/<path:filename>')
def serve_weapon_captures(filename):
    return send_from_directory(os.path.join(parent_dir, 'weapon_captures'), filename)




# Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ÙˆÙŠØ¨ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙØ­Ø§Øª
@app.route('/')
def index():
    """ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    return render_template('index.html')




@app.route('/live-broadcast.html')
def live_broadcast():
    """ØªÙ‚Ø¯ÙŠÙ… ØµÙØ­Ø© Ø§Ù„Ø¨Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø±"""
    return render_template('live-broadcast.html')




@app.route('/analytics.html')
def analytics():
    """ØªÙ‚Ø¯ÙŠÙ… ØµÙØ­Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª"""
    return render_template('analytics.html')




@app.route('/alerts-log.html')
def alerts_log():
    """ØªÙ‚Ø¯ÙŠÙ… ØµÙØ­Ø© Ø³Ø¬Ù„ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª"""
    return render_template('alerts-log.html')




@app.route('/drone-management.html')
def drone_management():
    """ØªÙ‚Ø¯ÙŠÙ… ØµÙØ­Ø© Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø·Ø§Ø¦Ø±Ø§Øª Ø¨Ø¯ÙˆÙ† Ø·ÙŠØ§Ø±"""
    return render_template('drone-management.html')




@app.route('/interactive-map.html')
def interactive_map():
    """ØªÙ‚Ø¯ÙŠÙ… ØµÙØ­Ø© Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©"""
    return render_template('interactive-map.html')




@app.route('/reports.html')
def reports():
    """ØªÙ‚Ø¯ÙŠÙ… ØµÙØ­Ø© Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±"""
    return render_template('reports.html')




@app.route('/settings.html')
def settings():
    """ØªÙ‚Ø¯ÙŠÙ… ØµÙØ­Ø© Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª"""
    return render_template('settings.html')




@app.route('/users.html')
def users():
    """ØªÙ‚Ø¯ÙŠÙ… ØµÙØ­Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†"""
    return render_template('users.html')




# Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø®Ø§Ø¯Ù…
@app.route('/test_server')
def test_server():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø£Ù† Ø§Ù„Ø®Ø§Ø¯Ù… ÙŠØ¹Ù…Ù„"""
    return jsonify({
        'status': 'ok',
        'message': 'Server is running',
        'time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'static_folder': app.static_folder,
        'upload_folder': UPLOADS_FOLDER,
        'processed_folder': PROCESSED_FOLDER,
        'captures_folder': CAPTURES_FOLDER
    })




@app.route('/test_upload', methods=['POST'])
def test_upload():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª"""
    try:
        logger.info("Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø±ÙØ¹ - Ø§Ø³ØªÙ„Ø§Ù… Ø·Ù„Ø¨")
        if 'file' not in request.files:
            logger.warning("Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø±ÙØ¹ - Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ù ÙÙŠ Ø§Ù„Ø·Ù„Ø¨")
            return jsonify({'error': 'No file in request'}), 400
            
        test_file = request.files['file']
        if test_file.filename == '':
            logger.warning("Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø±ÙØ¹ - Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù ÙØ§Ø±Øº")
            return jsonify({'error': 'Empty filename'}), 400
        
        logger.info(f"Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø±ÙØ¹ - Ø§Ø³ØªÙ„Ø§Ù… Ø§Ù„Ù…Ù„Ù: {test_file.filename}, Ø§Ù„Ù†ÙˆØ¹: {test_file.content_type}")
        return jsonify({
            'success': True,
            'message': f'Received file: {test_file.filename}, type: {test_file.content_type}'
        })
    except Exception as e:
        logger.error(f"Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø±ÙØ¹ - Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}")
        return jsonify({'error': str(e)}), 500




# Ù†Ù‚Ø§Ø· Ù†Ù‡Ø§ÙŠØ© API
@app.route('/api/streams', methods=['GET'])
def get_streams():
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨Ø« Ø§Ù„Ù†Ø´Ø·Ø©"""
    return jsonify({
        'streams': list(active_streams.values())
    })




@app.route('/api/start-stream', methods=['POST'])
def api_start_stream():
    """Ø¨Ø¯Ø¡ Ø¨Ø« Ø¬Ø¯ÙŠØ¯ Ø¹Ø¨Ø± API"""
    try:
        data = request.json or {}
        logger.info(f"Ø¨ÙŠØ§Ù†Ø§Øª Ø·Ù„Ø¨ Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø«: {data}")
        
        # Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ÙƒÙ„Ø§ Ø§Ù„Ø´ÙƒÙ„ÙŠÙ† (camera_id Ù…Ù† Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ© Ø£Ùˆ source_type Ù…Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚)
        camera_id = data.get('camera_id')
        source_type = data.get('source_type', 'webcam')
        source_path = data.get('source_path')
        rtsp_url = data.get('rtsp_url')
        name = data.get('name')
        
        # Ø¥Ø°Ø§ ØªÙ… ØªÙˆÙÙŠØ± camera_id ÙˆØ¨Ø¯ÙˆÙ† Ø§Ø³Ù…ØŒ Ø§Ø³ØªØ®Ø¯Ù…Ù‡ ÙƒØ§Ø³Ù…
        if camera_id and not name:
            name = camera_id
        
        stream_id = start_stream(source_type, source_path, rtsp_url, name)
        
        logger.info(f"Ø¨Ø¯Ø£ Ø§Ù„Ø¨Ø«: {stream_id} Ù„Ù„ÙƒØ§Ù…ÙŠØ±Ø§: {name or camera_id}")
        
        return jsonify({
            'stream_id': stream_id,
            'status': 'starting',
            'message': f'Stream {source_type} started',
            'camera_id': camera_id  # Ø¥Ø±Ø¬Ø§Ø¹ camera_id Ù„Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„ÙŠÙ‡ ÙÙŠ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
        })
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø«: {str(e)}")
        return jsonify({
            'error': str(e),
            'message': 'Failed to start stream'
        }), 500




@app.route('/api/stop-stream/<stream_id>', methods=['POST'])
def api_stop_stream(stream_id):
    """Ø¥ÙŠÙ‚Ø§Ù Ø¨Ø« Ø¹Ø¨Ø± API"""
    if stream_id not in active_streams:
        return jsonify({'error': 'Stream not found'}), 404
    
    success = stop_stream(stream_id)
    
    if success:
        return jsonify({
            'status': 'success',
            'message': f'Stream {stream_id} stopped'
        })
    else:
        return jsonify({
            'error': 'Failed to stop stream'
        }), 500




@app.route('/api/stop-stream', methods=['POST'])
def api_stop_stream_by_data():
    """Ø¥ÙŠÙ‚Ø§Ù Ø¨Ø« Ø¹Ø¨Ø± API Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª POST"""
    data = request.json or {}
    stream_id = data.get('stream_id')
    
    if not stream_id:
        return jsonify({'error': 'No stream_id provided'}), 400
        
    if stream_id not in active_streams:
        return jsonify({'error': 'Stream not found'}), 404
    
    success = stop_stream(stream_id)
    
    if success:
        return jsonify({
            'status': 'success',
            'message': f'Stream {stream_id} stopped'
        })
    else:
        return jsonify({
            'error': 'Failed to stop stream'
        }), 500




@app.route('/api/stream/<stream_id>', methods=['GET'])
def get_stream(stream_id):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨Ø«"""
    if stream_id not in active_streams:
        return jsonify({'error': 'Stream not found'}), 404
    
    return jsonify(active_streams[stream_id])




# API Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
@app.route('/upload', methods=['POST'])
def upload_video():
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±ÙØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ"""
    try:
        logger.info("Ø§Ø³ØªÙ„Ø§Ù… Ø·Ù„Ø¨ Ø±ÙØ¹ ÙÙŠØ¯ÙŠÙˆ")
        
        if 'video' not in request.files:
            logger.warning("Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ù ÙÙŠØ¯ÙŠÙˆ Ù…Ù‚Ø¯Ù…")
            return jsonify({'error': 'No video file provided'}), 400
        
        video_file = request.files['video']
        if video_file.filename == '':
            logger.warning("Ø§Ø³Ù… Ù…Ù„Ù ÙØ§Ø±Øº")
            return jsonify({'error': 'Empty filename'}), 400
        
        # Ø·Ø¨Ø§Ø¹Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù
        logger.info(f"Ø§Ø³ØªÙ„Ø§Ù… Ø§Ù„Ù…Ù„Ù: {video_file.filename}, Ø§Ù„Ù†ÙˆØ¹: {video_file.content_type}")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø±Ù ÙØ±ÙŠØ¯ ÙˆØ­ÙØ¸ Ø§Ù„Ù…Ù„Ù
        task_id = str(uuid.uuid4())
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„
        os.makedirs(UPLOADS_FOLDER, exist_ok=True)
        
        video_path = os.path.join(UPLOADS_FOLDER, f"{task_id}_{video_file.filename}")
        logger.info(f"Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù ÙÙŠ: {video_path}")
        
        video_file.save(video_path)
        logger.info(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù: {video_path}")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…Ù‡Ù…Ø©
        tasks[task_id] = {
            'id': task_id,
            'filename': video_file.filename,
            'upload_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'status': 'uploaded',
            'progress': 0
        }
        
        # Ø¨Ø¯Ø¡ Ù…Ø¤Ø´Ø± ØªØ±Ø§Ø¨Ø· Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        processing_thread = Thread(target=process_video_thread, args=(video_path, task_id))
        processing_thread.daemon = True
        processing_thread.start()
        
        logger.info(f"Ø¨Ø¯Ø£Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù‡Ù…Ø©: {task_id}")
        
        return jsonify({
            'task_id': task_id,
            'message': 'Video uploaded and processing started'
        })
    
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø±ÙØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500




@app.route('/task/<task_id>')
def get_task(task_id):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù‡Ù…Ø© ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬"""
    if task_id not in tasks:
        return jsonify({'error': 'Task not found'}), 404
    
    return jsonify(tasks[task_id])




# Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø£Ø­Ø¯Ø§Ø« Socket.IO
@socketio.on('connect')
def handle_connect():
    logger.info(f"âœ… Ø§ØªØµÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ„: {request.sid}")




@socketio.on('disconnect')
def handle_disconnect():
    logger.info(f"âŒ Ø§Ù†Ù‚Ø·Ø¹ Ø§ØªØµØ§Ù„ Ø§Ù„Ø¹Ù…ÙŠÙ„: {request.sid}")




@socketio.on('start_stream')
def handle_start_stream(data):
    source_type = data.get('source_type', 'webcam')
    source_path = data.get('source_path')
    rtsp_url = data.get('rtsp_url')
    name = data.get('name')
    camera_id = data.get('camera_id')  # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ camera_id Ø¥Ø°Ø§ ØªÙ… ØªÙˆÙÙŠØ±Ù‡
    
    # Ø¥Ø°Ø§ ØªÙ… ØªÙˆÙÙŠØ± camera_id ÙˆÙ„ÙƒÙ† Ù„ÙŠØ³ Ù‡Ù†Ø§Ùƒ Ø§Ø³Ù…ØŒ Ø§Ø³ØªØ®Ø¯Ù…Ù‡ ÙƒØ§Ø³Ù…
    if camera_id and not name:
        name = camera_id
    
    logger.info(f"Socket.IO start_stream: {name} ({source_type})")
    
    stream_id = start_stream(source_type, source_path, rtsp_url, name)
    
    return {
        'stream_id': stream_id,
        'status': 'starting',
        'message': f'Stream {source_type} started',
        'camera_id': camera_id  # Ø¥Ø±Ø¬Ø§Ø¹ camera_id Ù„Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„ÙŠÙ‡ ÙÙŠ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
    }




@socketio.on('stop_stream')
def handle_stop_stream(data):
    stream_id = data.get('stream_id')
    
    if not stream_id or stream_id not in active_streams:
        return {'status': 'error', 'message': 'Invalid stream ID'}
    
    success = stop_stream(stream_id)
    
    if success:
        return {'status': 'success', 'message': f'Stream {stream_id} stopped'}
    else:
        return {'status': 'error', 'message': 'Failed to stop stream'}




@socketio.on('get_streams')
def handle_get_streams():
    return {'streams': list(active_streams.values())}




@socketio.on('get_tasks')
def handle_get_tasks():
    return {'tasks': list(tasks.values())}




# Ø¨Ø¯Ø¡ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
if __name__ == '__main__':
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„
    load_models()
    
    # ØªØ´ØºÙŠÙ„ ØªØ·Ø¨ÙŠÙ‚ Flask Ù…Ø¹ Socket.IO
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ø®Ø§Ø¯Ù… Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ÙØ° 5600...")
    socketio.run(app, host='0.0.0.0', port=5600, debug=True, allow_unsafe_werkzeug=True)